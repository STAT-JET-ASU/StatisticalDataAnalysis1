---
title: "Modeling & Regression<br>&#9654; Theory and Line Fitting"
author: "Author: Jill E. Thomley"
date: 'Updated: `r format(Sys.time(), "%A, %B %d, %Y @ %X")`'
output: 
  ioslides_presentation:
    logo: images/logoASU.jpg
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(comment = "", message = FALSE, warning = FALSE)
```

## Before We Begin...

These slides are not meant to be standalone information. You should take notes to flesh out the contents. I recommend that you create an R Markdown document where you can combine information and code from the slides and your own additional notes and explorations to make connections.

**Related Materials**

* Ch 9 of *Mathematical Statistics with Resampling and R, 2^nd^ Ed.*
* Ch 5 of [*Modern Dive*](https://moderndive.com/5-regression.html)
* Ch 6 of [*Modern Dive*](https://moderndive.com/6-multiple-regression.html)
* DataCamp [Modeling with Data in the Tidyverse](https://learn.datacamp.com/courses/modeling-with-data-in-the-tidyverse)


## Modeling

<h3 style="text-align: center;">What is a model?</h3>

<p style="text-align: center;"><img src="images/molecularmodel.jpg"></p>

<h3 style="text-align: center;">What is a *mathematical* model?</h3>


## A General Model

General modeling framework: $y$ is a function of one or more $x$ variables, plus some amount of random error.

$$y = f(\overrightarrow{x}) + \epsilon$$

* y is the *outcome*, *response*, or *predicted* variable

* x is an *explanatory* or *predictor* variable ("signal")

* $\epsilon$ is the unsystematic error component ("noise")

The function $f(\overrightarrow{x})$ can assume a variety of forms, depending on the system and relationships we are trying to model. Finding the right model is often an iterative process. We often assume that $\epsilon$ is a normally distributed random variable.


## Simple Linear Model

Our theoretical simple linear model has the following form, with $f(x)$ being the familiar equation of a straight line.

$$y = f(x) + \epsilon = \beta_0 + \beta_1 x + \epsilon$$

We use data to estimate the $\beta$ parameters. The observed value of the *ith* data point $y_i$ in the sample can be expressed in terms of the fitted line and a residual (deviation from the line).

$$y_i = b_0 + b_1 x_i + e_i$$

The fitted (predicted) value $\hat{y}_i$ is a point lying on the fitted line, obtained by plugging the value $x_i$ into the fitted line. 

$$\hat{y}_i = b_0 + b_1 x_i \rightarrow y_i = \hat{y}_i + e_i$$


## 

<p style="text-align: center;"><img src="https://miro.medium.com/max/700/1*uoGLR9T-6_1hIlPhu2d_rg.png"></p>

image source: https://towardsdatascience.com/when-your-regression-models-errors-contain-two-peaks-13d835686ca


## "Best Fit" (Method of Least Squares)

The residual for each data point is the difference between the observed value of y and the predicted value of y.

$$y_i - \hat{y}_i = (b_0 + b_1 x_i + e_i) - (b_0 + b_1 x_i) = e_i$$


In ***least squares*** modeling, the best-fit line has the properties:

* Passes through the point $(\bar{x}, \bar{y})$

* Sum of the residuals $\sum_{i=1}^{n} e_i$ is equal to zero

* Sum of the squared residuals $\sum_{i=1}^{n} {e_i}^2$ is minimized

The sum of squared residuals (SSE) is an important quantity in regression analysis and is used for several purposes.


## Interpreting Slope and Intercept

Mathematically, the y-intercept $\beta_0$ and its estimate $b_0$ represent the value of $\hat{y}$ when $x$ = 0. 

The intercept term may or may not have a useful interpretation in context, depending on whether $x$ = 0 is meaningful. 

The units of measure for the intercept are the same as the units of measure for the $y$ variable.

<hr>

Slope is a rate of change; how many units of measure does the $\hat{y}$ value change when $x$ changes by one unit.

Since slope is a rate ($\Delta y$ / $\Delta x$), the units of measure for slope are ($y$ units of measure / $x$ units of measure).


## Are People Squares?

Roman architect [Vitruvius](https://www.ancient.eu/Vitruvius/) wrote that a person's height is equal to their armspan. Leonardo da Vinci later illustrated this idea.

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("images/Vitruvian-Man-Edited-800.jpg")
```


## Our Vitruvian Model

A general model, if we use height to explain / predict armspan...

$$armspan = \beta_0 + \beta_1 (height) + \epsilon$$

If Vitruvius is correct, what is the model's slope and y-intercept?

<p style="text-align: center;">$armspan =$ _____ $+$ _____ $(height)$</p>

What would the value of $\epsilon$ be if all people were perfect squares?

Individual people are likely not perfect squares, but maybe they are squares *on average*, with some amount of random variation for each person. For example, the *average* armspan for everyone 5'9" tall is 5'9", but some people have longer/shorter armspans.

Or ... maybe height and armspan have a different relationship?


## Anthropometric Data

The datafile [anthropometric.csv](https://raw.githubusercontent.com/STAT-JET-ASU/Datasets/master/Instructor/anthropometric.csv) contains measurements from some of Dr. Thomley's previous stats students ([description](https://stat-jet-asu.github.io/Datasets/InstructorDescriptions/anthropometric.html)).

```{r, echo = FALSE}
anthro <- read.csv("https://raw.githubusercontent.com/STAT-JET-ASU/Datasets/master/Instructor/anthropometric.csv")
library(dplyr)
library(ggplot2)
library(moderndive)
```

```{r}
glimpse(anthro)
```


## Take a Sample

The dataset is large, so let's take a smaller sample to start with. The `sample_n` function comes from `dplyr`. Setting a seed value ensures we all have the same "random" sample.

```{r}
set.seed(1)
anthro_sample <- sample_n(anthro, 20)
head(anthro_sample)
```


## EDA First!

Conduct exploratory data analyses on the x and y variables to investigate their shape, center, spread, and outliers. 

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("images/magnifying-glass.png", dpi = 500)
```


## Scatterplot

```{r, fig.height = 3.4}
# using coord_fixed because x and y have the same scale
ggplot(anthro_sample, aes(x = height, y = armspan)) +
  geom_point() + 
  coord_fixed(xlim = c(60, 80), ylim = c(60, 80))
```


## Scatterplot with Fitted Line

```{r, fig.height = 3.4}
ggplot(anthro_sample, aes(x = height, y = armspan)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  coord_fixed(xlim = c(60, 80), ylim = c(60, 80))
```


## Visualize the Residuals

```{r, echo = FALSE}
residuals <- get_regression_points(lm(armspan ~ height, data = anthro_sample))
```

The dashed lines on the scatterplot represent $\bar{x}$ and $\bar{y}$.

```{r, fig.height = 3.4, echo = FALSE}
ggplot(anthro_sample, aes(x = height, y = armspan)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  geom_segment(residuals, mapping = aes(x = height, 
                                        xend = height, 
                                        y = armspan_hat, 
                                        yend = (armspan))) +
  geom_vline(xintercept = mean(anthro_sample$height), linetype = "dashed") +
  geom_hline(yintercept = mean(anthro_sample$armspan), linetype = "dashed") +
  coord_fixed(xlim = c(60, 80), ylim = c(60, 80))

residuals <- arrange(residuals, height)
residuals$residual
```


## Scatterplot with Fitted & Plotted Lines

```{r, fig.height = 3.1}
ggplot(anthro_sample, aes(x = height, y = armspan)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +               # fitted line
  geom_abline(intercept = 0, slope = 1, color = "red") + # theoretical
  coord_fixed(xlim = c(60, 80), ylim = c(60, 80))
```


## Different Samples, Varying Estimates

```{r}
# there are more efficient ways to do this, notice the pattern

size_n <- 20

sample01 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S01")
sample02 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S02")
sample03 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S03")
sample04 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S04")
sample05 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S05")
sample06 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S06")
sample07 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S07")
sample08 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S08")

eightsamps <- rbind(sample01, sample02, sample03, sample04, 
                    sample05, sample06, sample07, sample08)
```


## Plotting All the Estimated Lines

Each sample yields a different estimate, but similar in slope.

```{r, fig.height = 2.8}
ggplot(eightsamps, aes(x = height, y = armspan, color = samplenum)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  coord_fixed(xlim = c(60, 80), ylim = c(60, 80))
```


## Fitting the Linear Model

The `lm()` function fits the least squares model and computes many related statistics. Far more information is computed and stored than the output that is displayed by default.

```{r}
square_model <- lm(armspan ~ height, data = anthro_sample)

# default output is the regression coefficients b0 and b1
square_model
```


##

```{r}
summary(square_model) # summary() gives us more output
```


##

The `lm()` function produces a `list` as output. We can access the components using the `$` notation.

```{r}
str(square_model)
```


## Accessing Output Components

```{r}
square_model$coefficients

square_model$residuals
```


## Formatting Results---Coefficients

The package `moderndive` contains some "wrapper" functions to help easily access and format regression output for information and additional analysis.

```{r}
square_model_table <- get_regression_table(square_model)

square_model_table
```

This output is a `tibble`. Why is this potentially useful to us? 


## Accessing Results---Coefficients

We can also print the results in a formatted Markdown version. It looks somewhat different in reports versus slides.

```{r}
get_regression_table(square_model, digits = 2, print = TRUE)
```

<br>The CIs help estimate the true values of the slope and intercept for the population versus just our data. (*We will dive more deeply into CIs later in the course.*)

