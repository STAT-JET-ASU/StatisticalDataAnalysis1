---
title: "Modeling / Regression"
author: "Author: Jill E. Thomley"
date: 'Updated: `r format(Sys.time(), "%A, %B %d, %Y @ %X")`'
output: ioslides_presentation
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(comment = "", message = FALSE, warning = FALSE)
```

## What is modeling?

What is a model?

What is a *mathematical* model?

<p style="text-align: center;"><img src="images/molecularmodel.jpg"></p>

## Simple Linear Model

General modeling framework: y is a function of one or more x values, plus some amount of random error.

$$y = f(\overrightarrow{x}) + \epsilon$$

* y is the *outcome* or *predicted* variable

* x is an *explanatory* or *predictor* variable ("signal")

* $\epsilon$ is the unsystematic error component ("noise")

Our basic simple linear model has the following form, with $f(x)$ being the familiar equation of a straight line.

$$y = \beta_0 + \beta_1 x + \epsilon$$


## Estimating the Model

The previous model is a theoretical or hypothesized population model for a relationship between y and x. We then use data to estimate the $\beta$ parameters in the model.

The observed value of the *ith* data point $y_i$ in the dataset can be expressed in terms of the fitted line and a residual (its deviation from the fitted line).

$$y_i = b_0 + b_1 x_i + e_i$$

The predicted (or fitted) value of the line is a prediction made by plugging a value $x$ into the fitted line. 

$$\hat{y}_i = b_0 + b_1 x_i$$


## The "Best Fit" Line

The residual for each data point is the difference between the observed value of y and the predicted value of y.

$$y_i - \hat{y}_i = (b_0 + b_1 x_i + e_i) - (b_0 + b_1 x_i) = e_i$$


In least squares modeling, the best-fit line is the one that meets the following criteria.

Passes through the point $(\bar{x}, \bar{y})$.

Has sum of the residuals $\sum_{i=1}^{n} e_i$ equal to zero.

Has sum of the squared residuals $\sum_{i=1}^{n} {e_i}^2$ is minimized.

The sum of squared residuals (SSE) is an important quantity in regression analysis and is used for several purposes.


## Are People Squares?

Roman architect [Vitruvius](https://www.ancient.eu/Vitruvius/) wrote that a person's height is equal to their armspan. Leonardo da Vinci later illustrated this idea.

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("images/Vitruvian-Man-Edited-800.jpg")
```


## Our Model

A general model, if we use height to explain / predict armspan...

$$armspan = \beta_0 + \beta_1 (height) + \epsilon$$

If Vitruvius is correct, what is the model's slope and y-intercept?

$\beta_0 =$ 

$\beta_1 =$

What would $\epsilon$ be if people were perfect squares?

Individual people are likely not perfect squares, but maybe they are squares *on average*, with some amount of random variation for each individual. Or, maybe there is a different relationship.


## Anthropometric Data

The datafile [anthropometric.csv](https://raw.githubusercontent.com/STAT-JET-ASU/Datasets/master/Instructor/anthropometric.csv) contains measurements from some of Dr. Thomley's previous statistics students.

```{r, echo = FALSE}
anthro <- read.csv("https://raw.githubusercontent.com/STAT-JET-ASU/Datasets/master/Instructor/anthropometric.csv")
library(dplyr)
library(ggplot2)
```

```{r}
glimpse(anthro)
```


## Take a Sample

The dataset is large, so let's take a smaller sample to start with. The `sample_n` function comes from `dplyr`. Setting a seed value ensures we all have the same "random" sample.

```{r}
set.seed(1)
anthro_sample <- sample_n(anthro, 20)
head(anthro_sample)
```


## EDA First!

Conduct exploratory data analyses on the x and y variables to investigate their shape, center, spread, and outliers. 

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("images/magnifying-glass.png", dpi = 500)
```


## Scatterplot

```{r, fig.height = 4}
ggplot(anthro_sample, aes(x = height, y = armspan)) +
  geom_point() + coord_fixed(xlim = c(60, 80), ylim = c(60, 80))
```




