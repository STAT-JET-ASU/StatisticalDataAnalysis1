---
title: "Modeling / Regression"
author: "Author: Jill E. Thomley"
date: 'Updated: `r format(Sys.time(), "%A, %B %d, %Y @ %X")`'
output: ioslides_presentation
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(comment = "", message = FALSE, warning = FALSE)
```

## Modeling

What is a model?

What is a *mathematical* model?

<p style="text-align: center;"><img src="images/molecularmodel.jpg"></p>

## Simple Linear Model

General modeling framework: y is a function of one or more x values, plus some amount of random error.

$$y = f(\overrightarrow{x}) + \epsilon$$

* y is the *outcome* or *predicted* variable

* x is an *explanatory* or *predictor* variable ("signal")

* $\epsilon$ is the unsystematic error component ("noise")

Our basic simple linear model has the following form, with $f(x)$ being the familiar equation of a straight line.

$$y = \beta_0 + \beta_1 x + \epsilon$$


## Estimating the Model

The previous model is a theoretical or hypothesized population model for a relationship between y and x. We then use data to estimate the $\beta$ parameters in the model.

The observed value of the *ith* data point $y_i$ in the dataset can be expressed in terms of the fitted line and a residual (its deviation from the fitted line).

$$y_i = b_0 + b_1 x_i + e_i$$

The predicted (or fitted) value of the line is a prediction made by plugging a value $x$ into the fitted line. 

$$\hat{y}_i = b_0 + b_1 x_i$$


## The "Best Fit" Line

The residual for each data point is the difference between the observed value of y and the predicted value of y.

$$y_i - \hat{y}_i = (b_0 + b_1 x_i + e_i) - (b_0 + b_1 x_i) = e_i$$


In least squares modeling, the best-fit line is the one that...

Passes through the point $(\bar{x}, \bar{y})$.

Has sum of the residuals $\sum_{i=1}^{n} e_i$ equal to zero.

Has sum of the squared residuals $\sum_{i=1}^{n} {e_i}^2$ is minimized.

The sum of squared residuals (SSE) is an important quantity in regression analysis and is used for several purposes.


## Are People Squares?

Roman architect [Vitruvius](https://www.ancient.eu/Vitruvius/) wrote that a person's height is equal to their armspan. Leonardo da Vinci later illustrated this idea.

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("images/Vitruvian-Man-Edited-800.jpg")
```


## Our Model

A general model, if we use height to explain / predict armspan...

$$armspan = \beta_0 + \beta_1 (height) + \epsilon$$

If Vitruvius is correct, what is the model's slope and y-intercept?

$\beta_0 =$ 

$\beta_1 =$

What would $\epsilon$ be if people were perfect squares?

Individual people are likely not perfect squares, but maybe they are squares *on average*, with some amount of random variation for each individual. Or, maybe there is a different relationship.


## Anthropometric Data

The datafile [anthropometric.csv](https://raw.githubusercontent.com/STAT-JET-ASU/Datasets/master/Instructor/anthropometric.csv) contains measurements from some of Dr. Thomley's previous stats students ([description](https://stat-jet-asu.github.io/Datasets/InstructorDescriptions/anthropometric.html)).

```{r, echo = FALSE}
anthro <- read.csv("https://raw.githubusercontent.com/STAT-JET-ASU/Datasets/master/Instructor/anthropometric.csv")
library(dplyr)
library(ggplot2)
library(moderndive)
```

```{r}
glimpse(anthro)
```


## Take a Sample

The dataset is large, so let's take a smaller sample to start with. The `sample_n` function comes from `dplyr`. Setting a seed value ensures we all have the same "random" sample.

```{r}
set.seed(1)
anthro_sample <- sample_n(anthro, 20)
head(anthro_sample)
```


## EDA First!

Conduct exploratory data analyses on the x and y variables to investigate their shape, center, spread, and outliers. 

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("images/magnifying-glass.png", dpi = 500)
```


## Scatterplot

```{r, fig.height = 4}
ggplot(anthro_sample, aes(x = height, y = armspan)) +
  geom_point() + coord_fixed(xlim = c(60, 80), ylim = c(60, 80))
```


## Scatterplot with Fitted Line

```{r, fig.height = 3.4}
ggplot(anthro_sample, aes(x = height, y = armspan)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  coord_fixed(xlim = c(60, 80), ylim = c(60, 80))
```


## Scatterplot with Fitted & Plotted Lines

```{r, fig.height = 3.1}
ggplot(anthro_sample, aes(x = height, y = armspan)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  coord_fixed(xlim = c(60, 80), ylim = c(60, 80))
```


## Different Samples, Varying Estimates

```{r}
# there are more efficient ways to do this, notice the pattern
size_n <- 20

sample01 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S01")
sample02 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S02")
sample03 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S03")
sample04 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S04")
sample05 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S05")
sample06 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S06")
sample07 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S07")
sample08 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S08")
sample09 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S09")
sample10 <- sample_n(anthro, size_n) %>% mutate(samplenum = "S10")

tensamps <- rbind(sample01, sample02, sample03, sample04, sample05,
                  sample06, sample07, sample08, sample09, sample10)
```


## Plotting All the Estimated Lines

```{r, fig.height = 3.4}
ggplot(tensamps, aes(x = height, y = armspan, color = samplenum)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  coord_fixed(xlim = c(60, 80), ylim = c(60, 80))
```


## Fitting the Linear Model

```{r}
square_model <- lm(height ~ armspan, data = anthro_sample)

square_model   # try str(square_model) and see what you get

square_model$coefficients
```


## Accessing the Output

```{r}
get_regression_table(square_model)   # from package moderndive
```

This output is a `tibble`, why is this potentially useful to us? 


##

```{r}
get_regression_points(square_model)   # from package moderndive
```

The output is a `tibble`, why is this potentially useful to us? 


## 

```{r}
summary(square_model)
```


## Parallel Slopes Model

```{r, fig.height = 4}
ggplot(anthro_sample, aes(x = height, y = armspan, color = gender)) +
  geom_point() + geom_smooth(method = "lm", se = FALSE)
```

## 

```{r}
square_model_gender <- lm(armspan ~ height + gender, data = anthro_sample)
get_regression_table(square_model_gender)
```

