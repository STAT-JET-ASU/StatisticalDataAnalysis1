---
title: "Think-Pair-Share: Type I versus Type II Error"
author: "SOLUTIONS // Jill E. Thomley"
date: '`r format(Sys.time(), "%A, %B %d, %Y @ %I:%M %p")`'
output: 
  html_document: 
    theme: yeti
    highlight: textmate
---

***
```{r globaloptions, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, comment = NA)
```

**Original Response Post:** 

1) Give an example of a situation in which a Type I error is worse than a Type II error.

2) Give an example of a situation in which a Type II error is worse than a Type I error.

3) Taking into account what you read about p-values last week, do you feel as if we can make a general statement about which type of error is generally better or worse? Discuss.

**Feedback:**

There were many good examples of Type I and Type II errors. A few people seemed to mix up the null and alternative hypothesis in their examples, some of which I commented on. It's important to remember that the null hypothesis is the "no effect" or "status quo" hypothesis, while the alternative hypothesis is the research hypothesis. For example, if doctors were interested in whether a new drug is an effective treatment for a particular disease, the null hypothesis would be that the drug has no effect of that it is no different than any current drug/treatment. The alternative would be that is does have an effect. Similarly, other examples of null hypotheses you may have seen include that a person is innocent, that they do not have a particular disease, or that there is no fire or emergency. These are all consistent with the idea that there is a burden of "proof" required before we reject certain ideas. Proof is in quotes here because, as this discussion highlights, when it comes to data we are working with incomplete evidence that may lead us to an incorrect decision.

Why is statistical hypothesis testing set up this way? If we write a null hypothesis as described above, then we can compute the probability that our results are due to chance alone (the p-value) and we can set the chance of Type I error at whatever level we choose. The typical level is $\alpha = 0.05$, but some researchers may instead use $\alpha = 0.01$ in order to reduce the chances of Type I error when making that type of error carries with it significant negative consequences. This is often the case in the medical field, where people are cautious about moving away from treatments with demonstrated effectiveness. Setting a smaller probability of Type I error makes it more difficult to reject $H_0$, which means we increase the chance of Type II error. It's a tradeoff.

What is the chance of Type II error in a given situation? We can never know the answer for sure, since $\beta$ depends on how wrong the null hypothesis is. For example, if there really is a difference between the population means of two groups, it is a small or a large difference? Often we speculate on a particular magnitude of difference to estimate the chance of Type II error. This can be useful to help choose the right sample size for an experiment, since sampling distributions of statistics depend on sample size. If we want to try to minimize both Type I and Type II error, then having a sufficiently large amount of data is important.  

There is no univeral answer about whether Type I or Type II error is better or worse. It depends on which has the more negative consequence in a given situation or context. However, given the general bias toward statistically significant results when it comes to publications, Type II errors may carry additional consequences in terms of sgaping the direction of future research. 


***
```{r}
sessionInfo()
```

