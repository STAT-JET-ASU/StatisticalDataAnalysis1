---
title: "Think-Pair-Share: Chihara & Hesterberg Ch02 EDA"
author: "SOLUTIONS // Jill E. Thomley"
date: '`r format(Sys.time(), "%A, %B %d, %Y @ %I:%M %p")`'
output: 
  html_document: 
    theme: yeti
    highlight: textmate
---

***
```{r globaloptions, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, comment = NA)
```

```{r}
library(tidyverse)
```

**Original Response Post:** Complete the following exercises from Chapter 2 of MSRR 2nd Ed., with the additional instructions. 

1) Exercise 2.2. Make a data frame with the eight numbers. Use mutate to perform the transformation and summarize to compute the means and medians. Paste your code and answer the question about whether the two means and two medians are equal. Be sure to state the summary values in your response.

This question is looking at the effect of a nonlinear transformation on the value of summary statistics, in this case a square root. If we take the mean and median of a set of data and apply a square root transformation to these statistics, is it the same as applying the square root transformation to the data first and them computing the statistic? 

```{r}
(summaries <- data.frame(x = c(1, 2, 4, 5, 6, 8, 11, 15)) %>% 
  mutate(sqrt_x = sqrt(x)) %>% 
  summarize(mean_x        = mean(x),
            mean_sqrt_x   = mean(sqrt_x),
            median_x      = median(x),
            median_sqrt_x = median(sqrt_x)))

sqrt(summaries$mean_x) == summaries$mean_sqrt_x
sqrt(summaries$median_x) == summaries$median_sqrt_x
```

**Exercise 1** asks the same question about logarithms. What would happen there? What if we used a linear transformation instead?


***
2) Exercise 2.6 (c) and (d). Use ggplot2 to make the boxplots and the quantile function to compute the quantiles. Paste the code and write your three comparative statements about the distributions. You may use both the graphical and quantitative results from (c) and (d) to make your comparisons.

**Remember:** Textbook datasets are available on our [class data files](https://stat-jet-asu.github.io/Datasets/DatasetList.html) page or in the `R` package `resampledata`.

```{r}
recidivism <- read_csv("https://raw.githubusercontent.com/STAT-JET-ASU/Datasets/master/Chihara/Recidivism.csv")

ggplot(recidivism, aes(x = Type, y = Days)) +
  geom_boxplot() +
  scale_y_continuous(breaks = seq(0, 1500, 100))
```

"When a person is released from prison, will he or she relapse into criminal behavior and be sent back? The state of Iowa tracks offenders over a 3-year period and records the number of days until recidivism for those who are readmitted to prison. The Department of Corrections uses this recidivism data to determine whether or not their strategies for preventing offenders from relapsing into criminal behavior are effective." (MSRR, 2nd Ed., p. 4)

There are three categories in the variable `Type`: New (committed a new crime), Tech (committed a technical violation, such as a parole violation), or No Recidivism. The variable `Days` refers to the number of days to recidivism (NA if no recidivism).

**Some comparisons based on the boxplots, numerical values are estimates:**

* The distribution of days in the New group seems to be fairly symmetric, while the distribution of the number of days in the Tech group is more right skewed.

* About 25% of people who committed new crimes did so before approximately 300 days post-release, versus 50% of people who committed technical violations.

* About 50% of people who committed new crimes did so before approximately 500 days post-release, versus 75% of people who committed technical violations.

* There is a difference of about 200 days between the medians of the New group (about 500 days) and the Tech group (about 300 days).

* The maximum number of days to recidivism was about 1100 in both the New and Tech groups, but this value was an outlier in the technical violation group. 

* The range for both groups spans roughly the entire possible timespan of 0-3 years, though the minimum time for the New group (about 20 days) is slightly longer than the minimum time for the Tech group (appears to be 0 days).

* The IQR in the new crimes group (about 450 days) is larger than the IQR in the technical violations group (about 325 days). 

* Zero days to recidivism and no recidivism are different, so there are no data values for anyone in the no recidivism group, whereas the technical violation groups seems to have some near-zero values (which would be people who went back to jail almost immediately). 

* When taken as a group, people who go back to prison on technical violations tend to do so sooner than those who commit new crimes. Omitting outliers, all values in the five-number-summary are larger for the New versus Tech groups. 

```{r}
quantile(recidivism$Days, c(0.25, 0.50, 0.75), na.rm = TRUE)

recidivism %>% 
  summarize(Q1 = quantile(Days, 0.25, na.rm = TRUE),
            Q2 = quantile(Days, 0.50, na.rm = TRUE),
            Q3 = quantile(Days, 0.75, na.rm = TRUE))
```

If we found the quantiles by `Type` to go with the boxplots. This would allow us to make more precise statements than the ones above, since we would have the exact values of the quartiles.

```{r}
recidivism %>% 
  group_by(Type) %>% 
  summarize(Q1 = quantile(Days, 0.25, na.rm = TRUE),
            Q2 = quantile(Days, 0.50, na.rm = TRUE),
            Q3 = quantile(Days, 0.75, na.rm = TRUE))
```

We could also use `boxplot.stats()` for the two groups to find the values used to make the box and outliers. That would help us to refine the statements above using more than "about".

```{r}
boxplot.stats(filter(recidivism, Type == "New")$Days)
boxplot.stats(filter(recidivism, Type == "Tech")$Days)
```

Finally, we could also convert these data to weeks, months, or years, which might be a more natural summary for someone who is studying this problem and presenting a report. However, that might be challenging with this limited data since releases happened in different months in 2010 with differing numbers of days, etc.

***
3) How could you design a simulation to help test/solve Exercise 2.9? You do not need to write any code, just describe in words what the process or steps might be.

The actual proof for the mean is a matter of algebra. This applies whether or not the values are ordered $x_1 < x_2 < x_3 \ldots < x_n$ and $y_1 < y_2 < y_3 \ldots < y_n$.

$$\bar{w} = \frac{w_1 + w_2 + \ldots + w_n}{n} =$$

$$\frac{x_1 + y_1 + x_2 + y_2 + \ldots + x_n + y_n}{n} =$$

$$\frac{x_1 + x_2 + \ldots + x_n}{n} + \frac{y_1 + y_2 + \ldots + y_n}{n} =$$

$$\bar{x} + \bar{y} $$

The proof for the median is a matter of logic and an extension of the proof for the mean. 

* If $x_1 < x_2 < x_3 \ldots < x_n$ and $y_1 < y_2 < y_3 \ldots < y_n$, then $w_1 < w_2 < w_3 \ldots < w_n$. Why is that? The relation $x_1 \leq x_2$ implies $x_1 + c \leq x_2 + c$. If we add instead $y_1 \leq y_2$ to the two sides, then $x_1 + y_1 \leq x_2 + y_2$.

* If n is an odd number, the median is the data value found in ordered location $i = (n + 1) / 2$. For example, if there are $n = 7$ data points, the median is the 4th ordered data point. Thus the median of $x$ is $x_{(n + 1) / 2}$, the median of $y$ is $y_{(n + 1) / 2}$, and since order is preserved the median of $w$ must be $w_{(n + 1) / 2} = x_{(n + 1) / 2} + y_{(n + 1) / 2}$.

* If n is an even number, the median is the average (mean) of the middle two ordered data values $i = n/2$ and $i = n/2 + 1$. For example, the middle two data values when $n = 8$ are the 4th and 5th. As we saw above, the average of a sum is the sum of the averages and the median of $w$ is the therefpre sum of the medians of $x$ and $y$.


How can we do this with simulation? That is trickier. Because of the way computers carry out mathematical operations and store quantitative values, we can encounter relatively small but critical rounding errors (see for example the phenomena of *catastrophic cancellation*). R uses double-precision floating point numbers, which give us about 16 decimal places. The simulation code below should *theoretically* always result in equal means and medians, given what was mathematically demonstrated above. However, it does *not* for means due to representational limitations in the R calculations How can see that in the simulation output? Why is it seemingly not a problem for the median?

```{r}
n <- 10
x <- sort(runif(n)) # random sample from the uniform distribution
y <- sort(runif(n)) # random sample from the uniform distribution
w <- x + y 
mean(x) + mean(y) == mean(w)
median(x) + median(y) == median(w)
```

```{r}
n <- 10 # sample size for x and y
numsims <- 100 # number of simulations
meandiff <- numeric(numsims)
mediandiff <- numeric(numsims)
for (i in 1:numsims) {
  x <- sort(runif(n)) # random sample from the uniform distribution
  y <- sort(runif(n)) # random sample from the uniform distribution
  w <- x + y 
  meandiff[i]   <- mean(x) + mean(y) - mean(w)
  mediandiff[i] <- median(x) + median(y) - median(w)
}
meandiff
mediandiff
```





***
```{r}
sessionInfo()
```

