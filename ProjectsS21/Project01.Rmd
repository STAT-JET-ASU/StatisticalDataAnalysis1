---
title: "Project 1: EDA & Modeling"
author: "PUT YOUR NAME HERE"
date: '`r format(Sys.time(), "%A, %B %d, %Y @ %I:%M %p")`'
output: 
  html_document: 
    theme: cosmo
    highlight: textmate
---

```{r globaloptions, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, comment = NA)
```

## {.tabset}

### Setup

#### Packages Used

Load all packages used in this project.

```{r}
# load packages here
```

#### Datasets Used

Read in all datasets used in this project and use `glimpse()` to visualize their structures.

**Fast Food Dataset**

```{r}
# read in data here
```

**Cookout Burgers Dataset**

```{r}
# read in data here
```

**Galton Heights Dataset**

```{r}
# read in data here
```

**Vietnam Draft Dataset**

```{r}
# read in data here
```

**Names of Collaborators**

List the names of anyone you worked with on this project. If there was no one, put NONE.

Names: 

#### Session Information

```{r}
sessionInfo()
```

<hr>


### READ.ME

#### Template & Grading

Complete all parts of this project using the provided template. Submit both the `.Rmd` and `.html` files as directed on AsULearn. The project will be scored with the provided rubric. The questions are equally weighted in terms of points, but they may not all be of equal difficulty for any given student. Spelling and grammar count, as does `R` programming quality.

#### General Plot Format

All plots must have an informative title and descriptive axis labels. For example, do not use the default variable names that appear, as these are often ambiguous. Include units of measure in axis labels. Colors, themes, and other embellishments are optional and welcome, but if you choose to use them they should not detract from the clarity and usefulness of the plot.

#### Support for Answers

In all cases, support your discussion answers using specific values and information from summaries, models, and/or plots. Do not refer vaguely to "the numbers" in your discussions. Use appropriate statistical vocabulary, concepts, and quantities so that a reader who cannot see your models, summaries, or plots could still reasonably understand your explanations.

#### Clean it All Up!

While you are working on the various data analyses and visualizations, you may try methods or code that in the end are not correct or not needed for the problem. Before turning in your project, remove extraneous content. It makes your work difficult to interpret and grade otherwise. Also, if the code is genuinely irrelevant or (even worse) produces a wrong answer, you will be marked down.

#### Claim Your Work!

Do not forget to put your name in the `YAML` header. Even though you are submitting on AsULearn, I will be opening the `.html` files in a browser or the `.Rmd` files in RStudio, so it is helpful to have your name on these documents so student submissions do not get mixed up.

#### Test the Knitting!

Do not wait until the last minute to try knitting your document. Even if everything seems to work as you are completing problems, knitting can reveal previously undiscovered mistakes and missing packages, code, etc. Knit early, knit often! Knitting can also make your results easier to see at times. Some output and formatting looks different in RStudio than when knitted.

#### Peer Collaboration

You are welcome and encouraged to work on the project with your classmates in either of Dr. Thomley's sections, in terms of discussing problems, concepts, and code, or comparing answers to see if you have obtained similar results. You may **NOT** split the project into parts with someone else and put it all together at the end so that you have not engaged fully with all the problems, since this project is intended to assess **your** knowledge. If I suspect this has occurred and find proof of that fact, you will receive a zero on the project and be barred from future collaboration.

**IF YOU ARE UNSURE ABOUT WHETHER SOMETHING IS ALLOWED OR HAVE ANY OTHER QUESTIONS, PLEASE ASK!**

<hr>


### Fast Food EDA

**The Context**

Nutritionists recommend against eating fast food because it is high in sodium, saturated fat, trans fat, and cholesterol. Eating too much fast food over a long period of time can lead to health problems, such as high blood pressure, heart disease, or obesity. Many fast-food meals contain more than an entire day's worth of recommended calories and fat! Several websites track the nutritional information of fast food. Read the [description](https://stat-jet-asu.github.io/Datasets/InstructorDescriptions/fastfood2017.html) of two different datasets, which you will use in this analysis and the next. You can access the data files from the description page.

**The Problem**

How much total fat, saturated fat, and trans fat are in some common fast food items? Let's find out!

**Exploration 1**

Create a frequency table of `restaurant` to discover which fast food restaurants are included in the dataset (and how many total items there are from each restaurant).

```{r}
# code here, add more chunks as needed
```

**Exploration 2**

Create a frequency and relative frequency (proportion) table of `type` to discover which categories of fast food items are included in the dataset (and how many there are of each).

```{r}
# code here, add more chunks as needed
```

**Exploration 3**

Create a horizontal barplot of `type` to visualize which categories of fast food items are included in the dataset (and how many total items there are of each type).

```{r}
# code here, add more chunks as needed
```

**Exploration 4**

Compute n, mean, and standard deviation for `totalfat` by `type` and sort by descending mean.

```{r}
# code here, add more chunks as needed
```

**Exploration 5**

Compute n, mean, and standard deviation for `saturatedfat` by `type` and sort by descending mean.

```{r}
# code here, add more chunks as needed
```

**Exploration 6**

Compute n, mean, and standard deviation for `transfat` by `type` and sort by descending mean.

```{r}
# code here, add more chunks as needed
```

**Discussion**

Question 1: According to US dietary guidelines, if you are following a standard 2000-calorie per day diet, your target range for total fat is 44 to 78 grams, with no more than 22 grams of saturated fat and no more than 2 grams of trans fat. Consider your summaries of the different kinds of fat in the different types of foods included in this sample. Fat values for each item were recorded on a per-serving basis. Which type of item seems to be the most unhealthy when considering all three kinds of fat? Explain. 

**ANSWER**

Question 2: In terms of the number of locations, McDonalds, Burger King, and Wendy's are the largest burger-selling chains in the United States. However, it is also true that Hardee's and Carl's Jr. are essentially the same chain with the same menu, just with different names in the eastern versus western United States. Given the restaurants and items included in this sample, do you feel that the data is generally representative of the population of fast food items in the United States? If not, what does it represent? Explain.

**ANSWER**

<hr>


### Fast Food Model

You will use the same datasets here that you read about and/or used in the **Fast Food EDA** problem.

**The Problem**

Many fast food menus now display the calorie counts for different items. Can we accurately predict the total amount of fat in fast food burgers from the number of calories?

**Data Reduction**

Create a dataset called `ffburgers` that contains only the observations for fast food burgers.

```{r}
# code here, add more chunks as needed
```

**Variable Renaming**

In the Cookout burgers dataset, rename the variable `fat` to `totalfat` to match the `ffburgers` dataset.

```{r}
# code here, add more chunks as needed
```

**Linear Modeling**

We want to predict total fat using a least-squares linear function of total calories.

* Create a plot of the fat versus calories relationship. 
* Include a fitted least squares line on the scatterplot. 
* Fit the least squares linear model and display results.
    + regression coefficients
    + R^2^, MSE, RMSE, sigma

The Plot 

```{r}
# code here, add more chunks as needed
```

The Model

```{r}
# code here, add more chunks as needed
```

**Test Predictions**

Test your ability to generalize your model by using it to predict the amount of fat in the four Cookout burgers, which were not part of the original dataset you used to create the model.  

```{r}
# code here, add more chunks as needed
```

**Discussion of Results**

Question 1: How do the residuals from the Cookout burger predictions compare to typical residuals from the fitted model? Explain.

**ANSWER**

Question 2: Based on the residuals and residual summaries, does it seem as if we can we accurately predict the total amount of fat in fast food burgers from the number of calories? Explain.

**ANSWER**

<hr>


### Galton EDA

**The Context**

Francis Galton, a cousin of Charles Darwin, studied the relationship between parent heights and the heights of their offspring. His pioneering [1886 article](https://galton.org/essays/1880-1889/galton-1886-jaigi-regression-stature.pdf) is the first appearance of regression analysis in print. He originated many statistical terms and concepts, including regression, correlation, deviate, quartile, and percentile, plus median for the midpoint of a distribution. His colleague and frequent collaborator Karl Pearson developed the Pearson product-moment correlation coefficient from the work of Galton and Auguste Bravais. The `galtonheightdata` dataset was [created under the direction of Dr. James A. Hanley](http://www.medicine.mcgill.ca/epidemiology/hanley/galton/) using Galton's original paper notebooks. The "female statures" are in their raw (untransmuted) form. You can access the data file from the [description](https://stat-jet-asu.github.io/Datasets/InstructorDescriptions/galtonheightdata.html) page.

**The Problem**

As one of his many genetic studies, Galton compared the heights of children to the heights of their parents. Before this analysis, he multiplied all of the female children's heights by 1.08. About this data transformation, Galton explained, "In every case I transmuted the female statures to their corresponding male equivalents and used them in their transmuted form, so that no objection grounded on the sexual difference of stature need be raised when I speak of averages." What is the impact on the distributions of children's heights when Galton's "transmutation" is used?

**Data Transformation**

Create a new variable in the dataset named `HeightT` by "transmuting" the `Height` variable such that female heights are multipled by 1.08 while male heights remain the same. 

```{r}
# code here, add more chunks as needed
```

**Exploration 1**

Compute summary statistics (n, mean, standard deviation, five-number summary, IQR, and range) for the measured heights of male children and the measured heights of female children so you can compare the two groups.

```{r}
# code here, add more chunks as needed
```

**Exploration 2**

Create a density plot with `color = Gender` in the aesthetics to show overlapping densities for measured heights of male and female children.

```{r}
# code here, add more chunks as needed
```

**Exploration 3**

Compute summary statistics (n, mean, standard deviation, five-number summary, IQR, and range) for the transmuted heights of male children and the transmuted heights of female children so you can compare the two groups.

```{r}
# code here, add more chunks as needed
```

**Exploration 4**

Create a density plot with `color = Gender` in the aesthetics to show overlapping densities for transmuted heights of male and female children.

```{r}
# code here, add more chunks as needed
```

**Exploration 5**

In general, when we transform data via multiplication by a constant, the mean and standard deviation of the transformed data are the same as if we multiplied the mean and standard deviation of the original data by the same constant. Show that this is true, either in this case using the data for female children, or algebraically using the formulas for mean and standard deviation.

```{r}
# code here, add more chunks as needed
# no code needed if solving algebraically
```

**Discussion of Results**

Was Galton's "transmutation" successful in essentially eliminating "the sexual difference of stature" in his dataset? Explain using information from both the summaries and the plots. 

**ANSWER:**

<hr>


### Galton Model

Here you will use the dataset with transmuted height that you created in **Galton EDA**.

**Data Transformation**

Create a new variable in the dataset named `Midparent` that contains the "midpoint" of mother and father heights for each observation. 

```{r}
# code here, add more chunks as needed
```

**Linear Modeling**

Galton sought to explain children's heights using their parents' heights (represented by the parental midpoint). You will conduct an analysis that is similar in intent but not mathematically identical---Galton did not use the least-squares method of line fitting. Use the children's transmuted heights and the parent mid-height as your model variables.

* Create a plot for child's height versus parent height. 
* Include a fitted least squares line on the scatterplot. 
* Compute the Pearson correlation between the variables.
* Fit the least squares linear model and display results.
    + regression coefficients
    + R^2^, MSE, RMSE, sigma

The Plot

```{r}
# code here, add more chunks as needed
```

The Model

```{r}
# code here, add more chunks as needed
```

**Test Predictions**

Dr. Thomley is female and 5'9" tall. Her mother is 5'6" and her father was 5'11". NBA great Wilt Chamberlain was male and 7'1". His dad, William Chamberlain, was 5'8". His mom, Olivia Ruth Johnson, was at 5'9". Use your model to predict Dr. Thomley's and Wilt Chamberlain's heights and calculate the residuals for these predictions.

```{r}
# code here, add more chunks as needed
```

**Discussion of Results**

Question 1: According to Galton, height was a good measurement to study because it is easy to obtain and, being the sum of more than a hundred individual body parts (e.g., legs, spine, skull), was resistant to variation in any one of the component parts. Overall, how much of the variation in offspring heights is *not* explained by average parent heights? What are some other factors that could explain the "error" variation in the offspring heights in the dataset and large differences in prediction like we saw?

**ANSWER**

Question 2: One assumption of least squares regression analysis is that all of the the observations are independent. In this case, that means each row of the dataset should be independent of (unrelated to) every other row of the dataset. Is that true in this case? Explain.

**ANSWER**

<hr>


### Vietnam EDA

**The Context**

In December 1969, the U.S. Selective Service System conducted a lottery to a determine draft order for eligible men in the upcoming year. The 366 days of the year (including February 29) were written on small pieces of paper. Each was placed in a plastic capsule, then the capsules were hand-mixed in a shoebox and dumped in a glass jar. Capsules were pulled out one at a time. The first number chosen was 258 (September 14). Anyone born on that day in 1944 through 1950 was assigned lottery number 1. The first 195 birthdates drawn were later called to serve in the order they were drawn. Some people asserted that the numbers were not randomly distributed through the year, in particular when you compared the first half of the year to the second half of the year. You can access the data files from the [description](https://stat-jet-asu.github.io/Datasets/InstructorDescriptions/vietnamdraft.html) page.

**The Problem**

The draft numbers for 1970 were drawn without replacement from the set $x = \{1, 2, 3, ..., 366\}$. The probability of drawing any draft number remaining in the pool at each step of the process was believed to be equal for all numbers, which would mean that all possible permutations of the 366 integers were equally likely---a fair process. Theoretically, the two halves of the year should tend to have similar distributions of numbers. It is impossible to prove whether the selection process was biased (non-random), but we can assess whether the outcome seems to be consistent with randomness. You will use EDA to examine the question of fairness for the 1970 draft.

**Data Reduction**

Create a dataset called `draft1970` that contains only observations for the 1970 draft and the variables `month`, `halfyear`, and `draftnumber`.

```{r}
# code here, add more chunks as needed
```

**Exploration 1**

Compute summary statistics (n, mean, standard deviation, five-number summary, IQR, and range) for the 1970 draft numbers. Then compute the same summaries to compare the first and second halves of the year.

Overall Summaries

```{r}
# code here, add more chunks as needed
```

Half-Year Summaries

```{r}
# code here, add more chunks as needed
```

**Exploration 2**

Create a boxplot to visualize the overall distribution of the 1970 draft numbers. Then create boxplots to compare the distributions of the first and second halves of the year.

Overall Distribution

```{r}
# code here, add more chunks as needed
```

Half-Year Distributions

```{r}
# code here, add more chunks as needed
```

**Exploration 3**

Rather than considering mean draft numbers, we could look a the proportion of birthdays whose numbers were called up for the draft. Overall, we know that 195/366 = 53.3% of birthdays were called up in 1970 (numbers 1 to 195 out of 366). Compute the proportion of birthdays in each half of the year that had draft numbers less than the overall median draft number. Also compute the proportion in each half that were drafted.

```{r}
# code here, add more chunks as needed
```

**Exploration 4**

A contemporary newspaper article (<a href="https://www.nytimes.com/1970/01/04/archives/statisticians-charge-draft-lottery-was-not-random.html">“Statisticians Charge Draft Lottery Was Not Random,”</a> New York Times, Jan . 4, 1970) that examined the draft lottery results included the following statement: "the average number for men born in January is 201, while the average number for December is 122...The average numbers for other months are: February, 203; March, 226; April, 204; May, 208; June, 196; July, 180; August, 173; September, 157; October, 182, and November, 149." Verify this using your data. Store these summaries to use in the first part of the **Vietnam Model** problem. 

```{r}
# code here, add more chunks as needed
```

**Discussion of Results**

Question 1: How does this analysis support the idea that the lottery was not truly random and was biased against those born in the second half of the year? Explain using information from all four of your explorations.

**ANSWER**

Question 2: Why is this analysis not *proof* of non-randomness? Explain.

**ANSWER**

<hr>


### Vietnam Model

You will use some of the datasets and summaries here that you created in the **Vietnam EDA** problem.

**Linear Model 1**

The newspaper article that examined the draft lottery results (discussed earlier in Vietnam EDA) also contained the following observation: "The general decline in the average monthly numbers can be seen if a graph is constructed, with the months from January through December along the horizontal axis and the average numbers plotted vertically. The points reach a high in March and then fall in an almost linear progression through December." Use your saved calculations from the Vietnam EDA problem.

* Create a plot like the one described in the newspaper. 
* Include a fitted least squares line on the scatterplot. 
* Fit the least squares linear model and display results.
    + regression coefficients
    + R^2^, MSE, RMSE, sigma

The Plot 

```{r}
# code here, add more chunks as needed
```

The Model

```{r}
# code here, add more chunks as needed
```

**Linear Model 2**

Creating regression models using averages is often frowned upon, since averages will always have less variability than the individual data points. The coefficients of the models may also be quite different, depending on the scatter of the individual points. Use the `draft1970` dataset here to conduct the same analysis using the individual data points.

* Create a plot like the one described in the newspaper. 
* Include a fitted least squares line on the scatterplot. 
* Fit the least squares linear model and display results.
    + regression coefficients
    + R^2^, MSE, RMSE, sigma

The Plot

```{r}
# code here, add more chunks as needed
```

The Model

```{r}
# code here, add more chunks as needed
```

**Discussion of Results**

Question 1: What would you expect the slope of the least-squares regression line, the Pearson correlation coefficient, and R^2^ to be if the 1970 draft lottery was truly random? Explain.

**ANSWER**

Question 2: How does this analysis support the idea that the lottery was not truly random and was biased against those born in the second half of the year? Explain using information from both of your models.

**ANSWER**

